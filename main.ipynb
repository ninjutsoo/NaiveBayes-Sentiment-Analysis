{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import csv\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "average = np.asarray([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = []\n",
    "all_negative_tweets = []\n",
    "all_tweets = []\n",
    "with open('StrictOMD.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        all_tweets.append(row)\n",
    "        if (row[0] == '4'):\n",
    "            all_positive_tweets.append(row[1])\n",
    "        else:\n",
    "            all_negative_tweets.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Amin\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Amin\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Amin\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\Amin\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package omw to\n[nltk_data]     C:\\Users\\Amin\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "source": [
    "# after first run, run from here to the end"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## read tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_positive_tweets = []\n",
    "all_negative_tweets = []\n",
    "all_tweets = np.asarray([])\n",
    "with open('StrictOMD.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        all_tweets = np.append(all_tweets, row[1])\n",
    "        if (row[0] == '4'):\n",
    "            all_positive_tweets.append(row[1])\n",
    "        else:\n",
    "            all_negative_tweets.append(row[1])\n",
    "# make positive and negative tweets random\n",
    "random.shuffle(all_tweets)\n",
    "train_x = all_tweets[100:]\n",
    "test_x = all_tweets[:100]\n",
    "train_y = []\n",
    "test_y = []\n",
    "for i in train_x:\n",
    "    if i in all_positive_tweets:\n",
    "        train_y = np.append(train_y, [1])\n",
    "    else:\n",
    "        train_y = np.append(train_y, [0])\n",
    "for i in test_x:\n",
    "    if i in all_positive_tweets:\n",
    "        test_y = np.append(test_y, [1])\n",
    "    else:\n",
    "        test_y = np.append(test_y, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            if(len(stem_word)>1):\n",
    "                tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word,y)\n",
    "\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([pair[0] for pair in freqs.keys()])# some words are in both negative an positive sentences\n",
    "V = len(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## number of all positive word and negative word with repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate N_pos, N_neg, V_pos, V_neg\n",
    "N_pos = N_neg = V_pos = V_neg = 0\n",
    "for pair in freqs.keys():\n",
    "    # if the label is positive (greater than zero)\n",
    "    if pair[1] > 0:\n",
    "        # increment the count of unique positive words by 1\n",
    "        V_pos += 1\n",
    "\n",
    "        # Increment the number of positive words by the count for this (word, label) pair\n",
    "        N_pos += freqs[pair]\n",
    "\n",
    "    # else, the label is negative\n",
    "    else:\n",
    "        # increment the count of unique negative words by 1\n",
    "        V_neg += 1\n",
    "\n",
    "        # increment the number of negative words by the count for this (word,label) pair\n",
    "        N_neg += freqs[pair]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute log prior. number of positive sentence on number of negative one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate D, the number of documents\n",
    "D = len(train_y)\n",
    "\n",
    "# Calculate D_pos, the number of positive documents\n",
    "D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
    "\n",
    "# Calculate D_neg, the number of negative documents\n",
    "D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n",
    "\n",
    "# Calculate logprior\n",
    "logprior = np.log(D_pos) - np.log(D_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute logliklihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    n = 0\n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglikelihood = {}\n",
    "for word in vocab:\n",
    "    # get the positive and negative frequency of the word\n",
    "    freq_pos = lookup(freqs,word,1)\n",
    "    freq_neg = lookup(freqs,word,0)\n",
    "\n",
    "    # calculate the probability that each word is positive, and negative\n",
    "    p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "    p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "    # calculate the log likelihood of the word\n",
    "    loglikelihood[word] = np.log(p_w_pos/p_w_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    word_l = process_tweet(tweet)\n",
    "    p = 0\n",
    "    p += logprior\n",
    "    for word in word_l:\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "\n",
    "    accuracy = 0  # return this properly\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            y_hat_i = 0\n",
    "\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1-error\n",
    "\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Naive Bayes accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Naive Bayes accuracy =\",\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = np.append(average, test_naive_bayes(test_x, test_y, logprior, loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.83, 0.84, 0.75, 0.76, 0.68, 0.8 , 0.82, 0.81, 0.82, 0.75, 0.84,\n",
       "       0.83, 0.84, 0.79, 0.8 , 0.79, 0.72, 0.81, 0.74, 0.77, 0.79, 0.8 ,\n",
       "       0.78, 0.74, 0.8 , 0.75, 0.86, 0.84, 0.77, 0.83, 0.86, 0.82, 0.76,\n",
       "       0.79, 0.77, 0.72, 0.76, 0.82, 0.85, 0.85, 0.83, 0.78, 0.79, 0.8 ,\n",
       "       0.83, 0.81, 0.8 , 0.76, 0.79, 0.83])"
      ]
     },
     "metadata": {},
     "execution_count": 852
    }
   ],
   "source": [
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "average for 50 run : 0.7953999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"average for\", len(average), \"run :\", np.mean(average))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}